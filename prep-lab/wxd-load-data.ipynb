{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf8dec3",
   "metadata": {},
   "source": [
    "## Load Wikipedia Data\n",
    "\n",
    "This notebook walks through the process of loading a wikipedia article into a watsonx.data relational database table. We use the [wikipedia python library](https://pypi.org/project/wikipedia/) to retrieve the wikipedia article. We then create a table in the database to store the article. Finally, we load the article into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080c362",
   "metadata": {},
   "source": [
    "#### Fetch wikipedia article\n",
    "\n",
    "Code is provided for searching wikipedia articles as well as fetching a specific article by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# search\n",
    "search_results = wikipedia.search(\"Climate\")\n",
    "search_results\n",
    "\n",
    "# view article summary\n",
    "article_summary = wikipedia.summary(search_results[0])\n",
    "article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0feaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# fetch wikipedia articles\n",
    "articles = {\n",
    "    'Climate change': None, \n",
    "    'Climate change mitigation': None\n",
    "}\n",
    "for k,v in articles.items():\n",
    "    article = wikipedia.page(k)\n",
    "    articles[k] = article.content\n",
    "    print(f\"Successfully fetched {k}\")\n",
    "\n",
    "print(f\"Successfully fetched {len(articles)} articles \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a38c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"de\")\n",
    "\n",
    "# fetch wikipedia articles\n",
    "articles_de = {\n",
    "    'Globale Erw√§rmung': None, \n",
    "    'Klimaschutz': None\n",
    "}\n",
    "for k,v in articles_de.items():\n",
    "    article_de = wikipedia.page(k)\n",
    "    articles_de[k] = article_de.content\n",
    "    print(f\"Successfully fetched {k}\")\n",
    "\n",
    "print(f\"Successfully fetched {len(articles)} articles \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d607c8-569c-4f53-bf4d-36a73699bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# search\n",
    "search_results = wikipedia.search(\"Climate\")\n",
    "search_results\n",
    "\n",
    "# view article summary\n",
    "article_summary = wikipedia.summary(search_results[0])\n",
    "article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96792953",
   "metadata": {},
   "source": [
    "## Load wikipedia article into watsonx.data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b70849-6f29-41dd-ad9d-2ea7acffd9bd",
   "metadata": {},
   "source": [
    "#### Connect to watsonx.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import urllib3\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # disable https warning\n",
    "\n",
    "# Because we are connecting to watsonx.data from the Jupyter Server hosted on the same vm as the watsonx.data services \n",
    "# Connecting to watsonx.data can be done with the local host credentials listed below:\n",
    "\n",
    "#these fields do not need to be changed \n",
    "\n",
    "LH_HOST_NAME='localhost'\n",
    "LH_PORT=8443\n",
    "LH_USER='ibmlhadmin'\n",
    "LH_PW='password'\n",
    "LH_CATALOG='tpch'\n",
    "LH_SCHEMA='tiny'\n",
    "\n",
    "quick_engine = create_engine(\n",
    "   f\"presto://{LH_USER}:{LH_PW}@{LH_HOST_NAME}:{LH_PORT}/{LH_CATALOG}/{LH_SCHEMA}\",\n",
    "   connect_args={\n",
    "    'protocol': 'https', \n",
    "    'requests_kwargs': {'verify': ssl.CERT_NONE }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf494b-ff35-4b45-8197-8c982d7a7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional connection check (this may take several seconds)\n",
    "#import pandas as pd \n",
    "#df = pd.read_sql('select * from tpch.tiny.customer limit 10', quick_engine)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723dc88-a7a3-4931-aa16-eb33e679a1be",
   "metadata": {},
   "source": [
    "### Create Schema in watsonx.data Hive Bucket to store wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f066915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "create_schema_result = pd.read_sql(\"\"\"\n",
    "\n",
    "CREATE SCHEMA hive_data.watsonxai WITH ( location = 's3a://hive-bucket/watsonx_ai')\n",
    "\n",
    "\"\"\", quick_engine)\n",
    "\n",
    "# Create table \n",
    "create_table_result = pd.read_sql(\"\"\"\n",
    "\n",
    "CREATE TABLE hive_data.watsonxai.wikipedia\n",
    "  (\n",
    "  \"id\" varchar,\n",
    "  \"text\" varchar,\n",
    "  \"title\" varchar  )\n",
    "WITH (\n",
    "     format = 'PARQUET'\n",
    "     )\n",
    "     \n",
    "\"\"\", quick_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc9ea9-67c0-40bb-8bb0-2f25dcecea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create table \n",
    "create_table_result = pd.read_sql(\"\"\"\n",
    "\n",
    "CREATE TABLE hive_data.watsonxai.wikipedia_de\n",
    "  (\n",
    "  \"id\" varchar,\n",
    "  \"text\" varchar,\n",
    "  \"title\" varchar  )\n",
    "WITH (\n",
    "     format = 'PARQUET'\n",
    "     )\n",
    "     \n",
    "\"\"\", quick_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and insert data\n",
    "\n",
    "\n",
    "# Chunk data\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "split_articles = {}\n",
    "for k,v in articles.items():\n",
    "    split_articles[k] = split_into_chunks(v, 225)\n",
    "\n",
    "\n",
    "# Insert data\n",
    "for article_title, article_chunks in split_articles.items():\n",
    "\n",
    "    for i, chunk in enumerate(article_chunks):\n",
    "            \n",
    "            escaped_chunk = chunk.replace(\"'\", \"''\").replace(\"%\", \"%%\")\n",
    "            insert_stmt = f\"insert into hive_data.watsonxai.wikipedia values ('{i+1}', '{escaped_chunk}', '{article_title}')\"\n",
    "            \n",
    "            with quick_engine.connect() as connection:\n",
    "                connection.execute(insert_stmt)\n",
    "            print(f\"{article_title} {i+1}/{len(article_chunks)} INSERTED\")\n",
    "            \n",
    "    print(f\"{article_title} DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca34819-0ade-4076-9516-dd19c5a58d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and insert data\n",
    "\n",
    "\n",
    "# Chunk data\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "split_articles = {}\n",
    "for k,v in articles_de.items():\n",
    "    split_articles[k] = split_into_chunks(v, 225)\n",
    "\n",
    "\n",
    "# Insert data\n",
    "for article_title, article_chunks in split_articles.items():\n",
    "\n",
    "    for i, chunk in enumerate(article_chunks):\n",
    "            \n",
    "            escaped_chunk = chunk.replace(\"'\", \"''\").replace(\"%\", \"%%\")\n",
    "            insert_stmt = f\"insert into hive_data.watsonxai.wikipedia_de values ('{i+1}', '{escaped_chunk}', '{article_title}')\"\n",
    "            \n",
    "            with quick_engine.connect() as connection:\n",
    "                connection.execute(insert_stmt)\n",
    "            print(f\"{article_title} {i+1}/{len(article_chunks)} INSERTED\")\n",
    "            \n",
    "    print(f\"{article_title} DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data inserted\n",
    "\n",
    "wiki_articles = pd.read_sql(\"select * from hive_data.watsonxai.wikipedia\", quick_engine)\n",
    "wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d45f3d-1742-4f7b-9c7f-ff641468209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data inserted\n",
    "\n",
    "wiki_articles_de = pd.read_sql(\"select * from hive_data.watsonxai.wikipedia_de\", quick_engine)\n",
    "wiki_articles_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1885008-1ee4-479b-a655-0b5348557fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
